{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install torch transformers easyocr pandas tqdm requests\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to download an image from a URL and convert to training dataset\nimport os\nimport torch\nimport pandas as pd\nimport requests\nfrom urllib3.util.retry import Retry\nfrom requests.adapters import HTTPAdapter\nfrom io import BytesIO\nfrom PIL import Image\nimport easyocr\nfrom tqdm import tqdm\nimport numpy as np\n\ndef download_image(url, retries=3, backoff_factor=0.3):\n    session = requests.Session()\n    retry = Retry(\n        total=retries,\n        read=retries,\n        connect=retries,\n        backoff_factor=backoff_factor,\n        status_forcelist=[500, 502, 503, 504],  # Retry on server errors\n        raise_on_status=False\n    )\n    adapter = HTTPAdapter(max_retries=retry)\n    session.mount('http://', adapter)\n    session.mount('https://', adapter)\n\n    try:\n        # Attempt to download the image\n        response = session.get(url, timeout=10)\n        response.raise_for_status()\n        image = Image.open(BytesIO(response.content)).convert('RGB')\n        return image\n    except Exception as e:\n        print(f\"Error downloading image from {url}: {e}\")\n    return None\n\n# Function to extract text from an image using EasyOCR\ndef extract_text_from_image(image, reader):\n    try:\n        result = reader.readtext(np.array(image), detail=0, paragraph=True)\n        return \" \".join(result)\n    except Exception as e:\n        print(f\"Error extracting text: {e}\")\n        return \"\"\n\n# Prepare the dataset for fine-tuning\ndef prepare_data(df, reader):\n    inputs = []\n    targets = []\n    skipped = 0\n\n    for idx, row in tqdm(df.iterrows(), total=df.shape[0]):\n        image_url = row['image_link']\n        entity_name = row['entity_name']\n        entity_value = row['entity_value']\n\n        # Validate target\n        if pd.isna(entity_value) or not isinstance(entity_value, str) or not entity_value.strip():\n            print(f\"Skipping entry with invalid target at index {idx}\")\n            skipped += 1\n            continue\n\n        # Download the image\n        image = download_image(image_url)\n        if image:\n            # Extract text from the image\n            context = extract_text_from_image(image, reader)\n            if not context.strip():\n                print(f\"No text extracted from image at index {idx}, skipping.\")\n                skipped += 1\n                continue\n        else:\n            print(f\"Image download failed at index {idx}, skipping.\")\n            skipped += 1\n            continue\n\n        question = f\"What is the {entity_name}?\"\n        input_text = f\"Question: {question} Context: {context}\"\n\n        inputs.append(input_text)\n        targets.append(entity_value)\n\n    print(f\"Data preparation completed. Skipped {skipped} entries due to invalid data.\")\n    return inputs, targets\n\n# Load the data\ndf = pd.read_csv('/kaggle/input/dataset/train.csv')  # Replace with your actual CSV path\ndf = df.head(5000)  # Use more data if available\n\n# Set up the OCR reader\nreader = easyocr.Reader(['en'], gpu=torch.cuda.is_available())\n\n# Prepare the data\ninputs, targets = prepare_data(df, reader)\n\n# Save the prepared dataset to disk\nprepared_data = {'inputs': inputs, 'targets': targets}\ntorch.save(prepared_data, 'prepared_data.pth')\n\nprint(\"Data preparation completed and saved to 'prepared_data.pth'.\")\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# function for training\nimport os\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\nfrom tqdm import tqdm\n\n# Set CUDA configuration to avoid memory fragmentation\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n\n# Enable cuDNN benchmark\ntorch.backends.cudnn.benchmark = True\n\n# Custom Dataset class for handling the inputs and targets\nclass EntityDataset(Dataset):\n    def __init__(self, inputs, targets, tokenizer, max_length=256):\n        self.inputs = inputs\n        self.targets = targets\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.inputs)\n\n    def __getitem__(self, index):\n        input_text = str(self.inputs[index])\n        target_text = str(self.targets[index])\n\n        input_encoding = self.tokenizer(\n            input_text,\n            max_length=self.max_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors=\"pt\",\n        )\n        target_encoding = self.tokenizer(\n            target_text,\n            max_length=32,  # Assuming the target is short\n            padding='max_length',\n            truncation=True,\n            return_tensors=\"pt\",\n        )\n\n        input_ids = input_encoding['input_ids'].flatten()\n        attention_mask = input_encoding['attention_mask'].flatten()\n        labels = target_encoding['input_ids'].flatten()\n\n        # Replace padding token id's of the labels by -100 so it's ignored by the loss\n        labels[labels == self.tokenizer.pad_token_id] = -100\n\n        return {\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask,\n            \"labels\": labels,\n        }\n\ndef main():\n    # Clear CUDA cache to free memory before loading the model\n    torch.cuda.empty_cache()\n\n    # Load the prepared data\n    prepared_data = torch.load('/kaggle/working/prepared_data.pth')\n    inputs, targets = prepared_data['inputs'], prepared_data['targets']\n\n    # Check for empty inputs or targets\n    valid_data = [(inp, tgt) for inp, tgt in zip(inputs, targets) if inp.strip() and tgt.strip()]\n    if not valid_data:\n        print(\"No valid data available for training.\")\n        return\n\n    inputs, targets = zip(*valid_data)\n\n    # Split data into training and validation sets\n    train_size = int(0.9 * len(inputs))\n    train_inputs, val_inputs = inputs[:train_size], inputs[train_size:]\n    train_targets, val_targets = targets[:train_size], targets[train_size:]\n\n    # Initialize tokenizer and model\n    tokenizer = T5Tokenizer.from_pretrained('google/flan-t5-base')  # Use 't5-small' if needed\n    model = T5ForConditionalGeneration.from_pretrained('google/flan-t5-base')\n\n    # Move model to GPU\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n\n    # Enable gradient checkpointing to save memory\n    model.gradient_checkpointing_enable()\n\n    # Create datasets and dataloaders\n    train_dataset = EntityDataset(train_inputs, train_targets, tokenizer)\n    val_dataset = EntityDataset(val_inputs, val_targets, tokenizer)\n\n    # Reduce batch size to fit in memory\n    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=1)\n\n    # Optimizer\n    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5)\n\n    # Implement gradient clipping\n    from torch.nn.utils import clip_grad_norm_\n\n    # Fine-tuning loop\n    for epoch in range(3):  # Adjust the number of epochs as necessary\n        model.train()\n        total_loss = 0\n        for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}\"):\n            optimizer.zero_grad()\n\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            labels = batch[\"labels\"].to(device)\n\n            outputs = model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                labels=labels,\n            )\n            loss = outputs.loss\n\n            # Check for NaN loss\n            if torch.isnan(loss):\n                print(\"NaN loss encountered, skipping this batch.\")\n                continue\n\n            total_loss += loss.item()\n\n            loss.backward()\n\n            # Clip gradients\n            clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n            optimizer.step()\n\n        avg_loss = total_loss / len(train_loader)\n        print(f\"Epoch {epoch + 1}, Average Loss: {avg_loss:.4f}\")\n\n        # Validation after each epoch\n        model.eval()\n        correct_predictions = 0\n        total_predictions = 0\n        val_loss = 0\n        with torch.no_grad():\n            for batch in tqdm(val_loader, desc=\"Validation\"):\n                input_ids = batch[\"input_ids\"].to(device)\n                attention_mask = batch[\"attention_mask\"].to(device)\n                labels = batch[\"labels\"].to(device)\n\n                outputs = model(\n                    input_ids=input_ids,\n                    attention_mask=attention_mask,\n                    labels=labels,\n                )\n                loss = outputs.loss\n\n                # Check for NaN loss\n                if torch.isnan(loss):\n                    print(\"NaN loss encountered in validation, skipping this batch.\")\n                    continue\n\n                val_loss += loss.item()\n\n                # Generate predictions\n                generated_ids = model.generate(\n                    input_ids=input_ids,\n                    attention_mask=attention_mask,\n                    max_length=32,\n                    num_beams=2,\n                )\n                preds = tokenizer.batch_decode(\n                    generated_ids, skip_special_tokens=True\n                )\n\n                # Decode labels while ignoring -100\n                labels_for_decoding = labels.clone()\n                labels_for_decoding[labels_for_decoding == -100] = tokenizer.pad_token_id\n                targets = tokenizer.batch_decode(\n                    labels_for_decoding, skip_special_tokens=True\n                )\n\n                # Compare predictions with ground truth\n                for pred, target in zip(preds, targets):\n                    if pred.strip().lower() == target.strip().lower():\n                        correct_predictions += 1\n                    total_predictions += 1\n\n        avg_val_loss = val_loss / len(val_loader)\n        accuracy = (correct_predictions / total_predictions * 100) if total_predictions > 0 else 0\n        print(f\"Validation Loss: {avg_val_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n\n    # Save the model\n    model.save_pretrained(\"fine_tuned_model\")\n    tokenizer.save_pretrained(\"fine_tuned_model\")\n    print(\"Model saved to 'fine_tuned_model' directory.\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# continue training\nimport os\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\nfrom tqdm import tqdm\n\n# Set CUDA configuration to avoid memory fragmentation\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n\n# Enable cuDNN benchmark\ntorch.backends.cudnn.benchmark = True\n\n# Custom Dataset class for handling the inputs and targets\nclass EntityDataset(Dataset):\n    def __init__(self, inputs, targets, tokenizer, max_length=256):\n        self.inputs = inputs\n        self.targets = targets\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.inputs)\n\n    def __getitem__(self, index):\n        input_text = str(self.inputs[index])\n        target_text = str(self.targets[index])\n\n        input_encoding = self.tokenizer(\n            input_text,\n            max_length=self.max_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors=\"pt\",\n        )\n        target_encoding = self.tokenizer(\n            target_text,\n            max_length=32,  # Assuming the target is short\n            padding='max_length',\n            truncation=True,\n            return_tensors=\"pt\",\n        )\n\n        input_ids = input_encoding['input_ids'].flatten()\n        attention_mask = input_encoding['attention_mask'].flatten()\n        labels = target_encoding['input_ids'].flatten()\n\n        # Replace padding token id's of the labels by -100 so it's ignored by the loss\n        labels[labels == self.tokenizer.pad_token_id] = -100\n\n        return {\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask,\n            \"labels\": labels,\n        }\n\ndef main():\n    # Clear CUDA cache to free memory before loading the model\n    torch.cuda.empty_cache()\n\n    # Load the prepared data\n    prepared_data = torch.load('/kaggle/working/prepared_data.pth')\n    inputs, targets = prepared_data['inputs'], prepared_data['targets']\n\n    # Check for empty inputs or targets\n    valid_data = [(inp, tgt) for inp, tgt in zip(inputs, targets) if inp.strip() and tgt.strip()]\n    if not valid_data:\n        print(\"No valid data available for training.\")\n        return\n\n    inputs, targets = zip(*valid_data)\n\n    # Split data into training and validation sets\n    train_size = int(0.9 * len(inputs))\n    train_inputs, val_inputs = inputs[:train_size], inputs[train_size:]\n    train_targets, val_targets = targets[:train_size], targets[train_size:]\n\n    # Load the saved model and tokenizer\n    tokenizer = T5Tokenizer.from_pretrained('fine_tuned_model')\n    model = T5ForConditionalGeneration.from_pretrained('fine_tuned_model')\n\n    # Move model to GPU\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n\n    # Enable gradient checkpointing to save memory\n    model.gradient_checkpointing_enable()\n\n    # Create datasets and dataloaders\n    train_dataset = EntityDataset(train_inputs, train_targets, tokenizer)\n    val_dataset = EntityDataset(val_inputs, val_targets, tokenizer)\n\n    # Reduce batch size to fit in memory\n    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=1)\n\n    # Optimizer\n    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5)\n\n    # Implement gradient clipping\n    from torch.nn.utils import clip_grad_norm_\n\n    # Fine-tuning loop (resuming from saved model)\n    for epoch in range(3):  # Adjust the number of epochs as necessary\n        model.train()\n        total_loss = 0\n        for batch in tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}\"):\n            optimizer.zero_grad()\n\n            input_ids = batch[\"input_ids\"].to(device)\n            attention_mask = batch[\"attention_mask\"].to(device)\n            labels = batch[\"labels\"].to(device)\n\n            outputs = model(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                labels=labels,\n            )\n            loss = outputs.loss\n\n            # Check for NaN loss\n            if torch.isnan(loss):\n                print(\"NaN loss encountered, skipping this batch.\")\n                continue\n\n            total_loss += loss.item()\n\n            loss.backward()\n\n            # Clip gradients\n            clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n            optimizer.step()\n\n        avg_loss = total_loss / len(train_loader)\n        print(f\"Epoch {epoch + 1}, Average Loss: {avg_loss:.4f}\")\n\n        # Validation after each epoch\n        model.eval()\n        correct_predictions = 0\n        total_predictions = 0\n        val_loss = 0\n        with torch.no_grad():\n            for batch in tqdm(val_loader, desc=\"Validation\"):\n                input_ids = batch[\"input_ids\"].to(device)\n                attention_mask = batch[\"attention_mask\"].to(device)\n                labels = batch[\"labels\"].to(device)\n\n                outputs = model(\n                    input_ids=input_ids,\n                    attention_mask=attention_mask,\n                    labels=labels,\n                )\n                loss = outputs.loss\n\n                # Check for NaN loss\n                if torch.isnan(loss):\n                    print(\"NaN loss encountered in validation, skipping this batch.\")\n                    continue\n\n                val_loss += loss.item()\n\n                # Generate predictions\n                generated_ids = model.generate(\n                    input_ids=input_ids,\n                    attention_mask=attention_mask,\n                    max_length=32,\n                    num_beams=2,\n                )\n                preds = tokenizer.batch_decode(\n                    generated_ids, skip_special_tokens=True\n                )\n\n                # Decode labels while ignoring -100\n                labels_for_decoding = labels.clone()\n                labels_for_decoding[labels_for_decoding == -100] = tokenizer.pad_token_id\n                targets = tokenizer.batch_decode(\n                    labels_for_decoding, skip_special_tokens=True\n                )\n\n                # Compare predictions with ground truth\n                for pred, target in zip(preds, targets):\n                    if pred.strip().lower() == target.strip().lower():\n                        correct_predictions += 1\n                    total_predictions += 1\n\n        avg_val_loss = val_loss / len(val_loader)\n        accuracy = (correct_predictions / total_predictions * 100) if total_predictions > 0 else 0\n        print(f\"Validation Loss: {avg_val_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n\n    # Save the model after fine-tuning\n    model.save_pretrained(\"fine_tuned_model\")\n    tokenizer.save_pretrained(\"fine_tuned_model\")\n    print(\"Model saved to 'fine_tuned_model' directory.\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predictor\nimport os\nimport torch\nimport pandas as pd\nimport requests\nfrom urllib3.util.retry import Retry\nfrom requests.adapters import HTTPAdapter\nfrom io import BytesIO\nfrom PIL import Image\nimport easyocr\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\nfrom tqdm import tqdm\nimport numpy as np\n\n# Function to download an image from a URL with retry logic and delay\ndef download_image(url, retries=3, backoff_factor=0.3):\n    session = requests.Session()\n    retry = Retry(\n        total=retries,\n        read=retries,\n        connect=retries,\n        backoff_factor=backoff_factor,\n        status_forcelist=[500, 502, 503, 504],  # Retry on server errors\n        raise_on_status=False\n    )\n    adapter = HTTPAdapter(max_retries=retry)\n    session.mount('http://', adapter)\n    session.mount('https://', adapter)\n\n    try:\n        # Attempt to download the image\n        response = session.get(url, timeout=10)\n        response.raise_for_status()\n        image = Image.open(BytesIO(response.content)).convert('RGB')\n        return image\n    except Exception as e:\n        print(f\"Error downloading image from {url}: {e}\")\n    return None\n\n# Function to extract text from an image using EasyOCR\ndef extract_text_from_image(image, reader):\n    try:\n        result = reader.readtext(np.array(image), detail=0, paragraph=True)\n        return \" \".join(result)\n    except Exception as e:\n        print(f\"Error extracting text: {e}\")\n        return \"\"\n\ndef main():\n    # Load the test data\n    test_df = pd.read_csv('/kaggle/input/qweqwe/test.csv')  # Replace with your test CSV file path\n    test_df=test_df[:10000]\n    # Ensure the test dataframe has the necessary columns\n    required_columns = ['image_link', 'entity_name']\n    missing_columns = [col for col in required_columns if col not in test_df.columns]\n    if missing_columns:\n        print(f\"Error: Test data is missing columns: {missing_columns}\")\n        return\n\n    # Set up the OCR reader\n    reader = easyocr.Reader(['en'], gpu=torch.cuda.is_available())\n\n    # Load the fine-tuned model and tokenizer\n    tokenizer = T5Tokenizer.from_pretrained('/kaggle/working/fine_tuned_model')\n    model = T5ForConditionalGeneration.from_pretrained('/kaggle/working/fine_tuned_model')\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n    model.eval()  # Set model to evaluation mode\n\n    # Prepare for predictions\n    predictions = []\n    for idx, row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Processing Test Data\"):\n        image_url = row['image_link']\n        entity_name = row['entity_name']\n\n        # Download the image\n        image = download_image(image_url)\n        if image:\n            # Extract text from the image\n            context = extract_text_from_image(image, reader)\n            if not context.strip():\n                print(f\"No text extracted from image at index {idx}\")\n                context = \"\"\n        else:\n            print(f\"Image download failed at index {idx}\")\n            context = \"\"\n\n        question = f\"What is the {entity_name}?\"\n        input_text = f\"Question: {question} Context: {context}\"\n\n        # Tokenize input text\n        input_encoding = tokenizer(\n            input_text,\n            max_length=256,\n            padding='max_length',\n            truncation=True,\n            return_tensors='pt',\n        )\n        input_ids = input_encoding['input_ids'].to(device)\n        attention_mask = input_encoding['attention_mask'].to(device)\n\n        # Generate prediction\n        with torch.no_grad():\n            generated_ids = model.generate(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                max_length=32,\n                num_beams=2,\n                early_stopping=True,\n            )\n            pred = tokenizer.decode(\n                generated_ids[0],\n                skip_special_tokens=True,\n                clean_up_tokenization_spaces=True\n            )\n            # Append the prediction to the list\n            predictions.append({\n                'image_link': image_url,\n                'entity_name': entity_name,\n                'predicted_value': pred\n            })\n\n    # Convert predictions to DataFrame\n    output_df = pd.DataFrame(predictions)\n\n    # Save to CSV\n    output_df.to_csv('sample_out.csv', index=False)\n    print(\"Predictions saved to 'sample_out.csv'\")\n\nif __name__ == '__main__':\n    main()\n","metadata":{},"execution_count":null,"outputs":[]}]}